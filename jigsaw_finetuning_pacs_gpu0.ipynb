{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cartoon-photo-sketch to art_painting - 31 jigsaw classes, split 0\n",
      "Using Caffe AlexNet\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "Linear\n",
      "AlexNetCaffePatches(\n",
      "  (features): Sequential(\n",
      "    (conv1): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (relu1): ReLU(inplace)\n",
      "    (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (norm1): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1)\n",
      "    (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=2)\n",
      "    (relu2): ReLU(inplace)\n",
      "    (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (norm2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1)\n",
      "    (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (relu3): ReLU(inplace)\n",
      "    (conv4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "    (relu4): ReLU(inplace)\n",
      "    (conv5): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "    (relu5): ReLU(inplace)\n",
      "    (pool5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (fc6): Linear(in_features=9216, out_features=1024, bias=True)\n",
      "    (relu6): ReLU(inplace)\n",
      "    (drop6): Dropout(p=0.5)\n",
      "  )\n",
      "  (jigsaw_classifier): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=32, bias=True)\n",
      "  )\n",
      "  (class_classifier): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "    (3): Linear(in_features=4096, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "Dataset size: train 7150, val 793, test 2048\n",
      "Step size: 24\n",
      "Saving to /home/enoon/code/2018/JigsawDA/myJigsaw/utils/../logs/patches/cartoon-photo-sketch_to_art_painting/eps30_bs128_lr0.001_class7_jigClass31_jigWeight0_TAll_245\n",
      "New epoch - lr: 0.001\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AlexNetCaffePatches' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-148fd05699bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                           i))\n\u001b[1;32m     48\u001b[0m         \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/code/2018/JigsawDA/myJigsaw/train_jigsaw.py\u001b[0m in \u001b[0;36mdo_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mval_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mtest_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/2018/JigsawDA/myJigsaw/train_jigsaw.py\u001b[0m in \u001b[0;36m_do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mjigsaw_logit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mjigsaw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjigsaw_logit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjig_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monly_non_scrambled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/2018/JigsawDA/myJigsaw/models/patch_based.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mfc7_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AlexNetCaffePatches' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from data import data_helper\n",
    "from data.data_helper import available_datasets\n",
    "from models import model_factory\n",
    "from optimizer.optimizer_helper import get_optim_and_scheduler\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "from utils.Logger import Logger\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from train_jigsaw import Trainer\n",
    "from utils import vis\n",
    "\n",
    "class Container():\n",
    "    pass\n",
    "\n",
    "args = Container()\n",
    "args.batch_size = 128\n",
    "args.n_classes = 7\n",
    "args.learning_rate = 0.001\n",
    "args.epochs = 30\n",
    "args.network = \"caffenet_patches\"\n",
    "args.val_size = 0.1\n",
    "args.tf_logger = True\n",
    "args.folder_name = \"patches\"\n",
    "args.jigsaw_n_classes = 31\n",
    "args.classify_only_sane = False\n",
    "args.jig_weight = 0.0\n",
    "args.bias_whole_image = None\n",
    "args.TTA = False\n",
    "args.train_all = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "source = sorted([\"photo\", \"cartoon\", \"sketch\", \"art_painting\"])\n",
    "# for args.TTA in [True, False]: \n",
    "for k, x in enumerate(source):\n",
    "    args.source = source[:k]+source[k+1:]\n",
    "    args.target = x\n",
    "    for i in range(3):\n",
    "        print(\"\\n%s to %s - %d jigsaw classes, split %d\" % (\"-\".join(args.source), \n",
    "                                                          args.target, \n",
    "                                                          args.jigsaw_n_classes,\n",
    "                                                          i))\n",
    "        trainer = Trainer(args, device)\n",
    "        trainer.do_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source = sorted([\"photo\", \"cartoon\", \"sketch\", \"art_painting\"])\n",
    "args.epochs = 30\n",
    "args.jigsaw_n_classes = 31\n",
    "for args.bias_whole_image in [None, 0.1, 0.3, 0.5]:\n",
    "    for args.classify_only_sane in [True, False]:\n",
    "        print(\"===============================\\n\")\n",
    "        for k, x in enumerate(source):\n",
    "            args.source = source[:k]+source[k+1:]\n",
    "            args.target = x\n",
    "            for i in range(3):\n",
    "                print(\"\\n%s to %s - %d jigsaw classes, split %d\" % (\"-\".join(args.source), \n",
    "                                                                  args.target, \n",
    "                                                                  args.jigsaw_n_classes,\n",
    "                                                                  i))\n",
    "                trainer = Trainer(args, device)\n",
    "                trainer.do_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source = sorted([\"photo\", \"cartoon\", \"sketch\", \"art_painting\"])\n",
    "for args.jigsaw_n_classes in [1,3,5,10,20]:\n",
    "    args.epochs = int((80/95)*args.jigsaw_n_classes+15)\n",
    "    for k, x in enumerate(source):\n",
    "        args.source = source[:k]+source[k+1:]\n",
    "        args.target = x\n",
    "        for i in range(3):\n",
    "            print(\"\\n%s to %s - %d jigsaw classes, split %d\" % (\"-\".join(args.source), \n",
    "                                                              args.target, \n",
    "                                                              args.jigsaw_n_classes,\n",
    "                                                              i))\n",
    "            trainer = Trainer(args, device)\n",
    "            trainer.do_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args, device)\n",
    "logger, model = trainer.do_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args, device)\n",
    "logger, model = trainer.do_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "print(100*(logger.val_acc[\"class\"][-1] + logger.val_acc[\"class\"][-2])/2.)\n",
    "vis.view_training(logger, \"%s->%s eps:%d jigweight:%.1f\" % (\"-\".join(args.source),\n",
    "                                                            args.target,args.epochs, args.jig_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "for k,v in logger.losses.items():\n",
    "    ax1.plot(v, label=k)\n",
    "    l = len(v)\n",
    "updates = l / len(logger.val_acc)\n",
    "print(updates)\n",
    "plt.legend()\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(0,l,int(updates)), logger.val_acc, label=\"Test acc\", c='g')\n",
    "plt.legend()\n",
    "plt.title(\"%s->%s eps:%d jigweight:%.2f\" % (str(source),target,epochs, jig_weight))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "for k,v in logger.losses.items():\n",
    "    ax1.plot(v, label=k)\n",
    "    l = len(v)\n",
    "updates = l / len(logger.val_acc)\n",
    "print(updates)\n",
    "plt.legend()\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(range(0,l,int(updates)), logger.val_acc, label=\"Test acc\", c='g')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_plt(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "conv1 = models.alexnet(pretrained=True).features[0] #model_ft.features[0]\n",
    "tmp = conv1.weight.cpu().data\n",
    "tmp = torchvision.utils.make_grid(tmp,normalize=True)\n",
    "plt.imshow(to_plt(tmp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "conv1 = model_ft.features[0]\n",
    "tmp = conv1.weight.cpu().data\n",
    "tmp = torchvision.utils.make_grid(tmp,normalize=True)\n",
    "plt.imshow(to_plt(tmp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(memory[\"train\"], label=\"train\")\n",
    "plt.plot(memory[\"val\"], label=\"val\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# iter_c = iter(train_datasets)\n",
    "\n",
    "# for x in range(5):\n",
    "#     tmp = next(iter_c)\n",
    "#     image = to_plt(tmp[0])\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from data.data_helper import get_val_dataloader, get_train_dataloader\n",
    "from os.path import join, dirname\n",
    "# from data.JigsawLoader import JigsawTestDataset\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "\n",
    "loader = get_val_dataloader(\"photo\",31,batch_size=10,multi=True)\n",
    "# loader, _ = get_train_dataloader([\"photo\"],31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_plt(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    return inp\n",
    "\n",
    "# dataset = JigsawTestDataset(\"\", join('data/txt_lists', 'dslr_train.txt'), patches=False, classes=31)\n",
    "# test = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True, num_workers=4, pin_memory=True, drop_last=False)\n",
    "iter_c = iter(loader)\n",
    "(tmp, v, c), d = next(iter_c)\n",
    "print(tmp[:,0].shape)\n",
    "\n",
    "for x in range(tmp.shape[1]):\n",
    "#     image = tmp[0, x]\n",
    "    image = torchvision.utils.make_grid(tmp[0, x],1,normalize=True)\n",
    "    plt.imshow(to_plt(image))\n",
    "    plt.show()\n",
    "    print(v[0,x])\n",
    "\n",
    "    \n",
    "# print(v.max(), v.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[d[d==k].shape for k in [0,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.JigsawLoader import JigsawDataset\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class JigsawTestDataset(JigsawDataset):\n",
    "    def __init__(self, *args, **xargs):\n",
    "        super().__init__(*args, **xargs)\n",
    "        self._augment_tile = transforms.Compose([\n",
    "#             transforms.RandomCrop(64),\n",
    "            transforms.Resize((75, 75), Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + '/' + self.names[index]\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        img = self._image_transformer(img)\n",
    "\n",
    "        w = float(img.size[0]) / self.grid_size\n",
    "        n_grids = self.grid_size ** 2\n",
    "        tiles = [None] * n_grids\n",
    "        for n in range(n_grids):\n",
    "            y = int(n / self.grid_size)\n",
    "            x = n % self.grid_size\n",
    "            tile = img.crop([x * w, y * w, (x + 1) * w, (y + 1) * w])\n",
    "            tile = self._augment_tile(tile)\n",
    "            tiles[n] = tile\n",
    "\n",
    "        data = torch.stack(tiles, 0)\n",
    "        return self.returnFunc(data), 0, int(self.labels[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = JigsawTestDataset(\"\", join('data/txt_lists', 'dslr_train.txt'), patches=False, classes=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perms = np.load(\"permutations_31.npy\")\n",
    "perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
