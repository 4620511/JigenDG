{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, numpy as np\n",
    "import argparse\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow # needs to call tensorflow before torch, otherwise crush\n",
    "from utils.Logger import Logger\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from models import model_factory\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "n_classes = 31\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class JigsawDataset(data.Dataset):\n",
    "    def __init__(self, data_path, txt_list, classes=1000):\n",
    "        self.data_path = data_path\n",
    "        self.names, _ = self.__dataset_info(txt_list)\n",
    "        self.N = len(self.names)\n",
    "        self.permutations = self.__retrive_permutations(classes)\n",
    "\n",
    "        self.__image_transformer = transforms.Compose([\n",
    "            transforms.Resize(256, Image.BILINEAR),\n",
    "            transforms.CenterCrop(255)])\n",
    "        self.__augment_tile = transforms.Compose([\n",
    "            transforms.RandomCrop(64),\n",
    "            transforms.Resize((75, 75), Image.BILINEAR),\n",
    "            transforms.ColorJitter(0.3, 0.3, 0.3, 0.3),\n",
    "            transforms.RandomGrayscale(0.3),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + '/' + self.names[index]\n",
    "\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        img = self.__image_transformer(img)\n",
    "\n",
    "        w = float(img.size[0]) / 3\n",
    "        tiles = [None] * 9\n",
    "        for n in range(9):\n",
    "            y = int(n / 3)\n",
    "            x = n % 3\n",
    "            tile = img.crop([x * w, y * w, (x+1)*w, (y+1)*w])\n",
    "            tile = self.__augment_tile(tile)\n",
    "            # Normalize the patches indipendently to avoid low level features shortcut\n",
    "            tiles[n] = tile \n",
    "\n",
    "        order = np.random.randint(len(self.permutations))\n",
    "        data = [tiles[self.permutations[order][t]] for t in range(9)]\n",
    "        data = torch.stack(data, 0)\n",
    "        return torchvision.utils.make_grid(data, 3, padding=False), int(order)   \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __dataset_info(self, txt_labels):\n",
    "        with open(txt_labels, 'r') as f:\n",
    "            images_list = f.readlines()\n",
    "\n",
    "        file_names = []\n",
    "        labels = []\n",
    "        for row in images_list:\n",
    "            row = row.split(' ')\n",
    "            file_names.append(row[0])\n",
    "            labels.append(int(row[1]))\n",
    "\n",
    "        return file_names, labels\n",
    "\n",
    "    def __retrive_permutations(self, classes):\n",
    "        all_perm = np.load('permutations_%d.npy' % (classes))\n",
    "        # from range [1,9] to [0,8]\n",
    "        if all_perm.min() == 1:\n",
    "            all_perm = all_perm - 1\n",
    "\n",
    "        return all_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "office_list = [\"amazon\",\"dslr\",\"webcam\"]\n",
    "list_paths = {dataset: \"/home/enoon/data/images/office/%s/train.txt\" % dataset for dataset in office_list}\n",
    "datasets = {dataset: JigsawDataset(\"\", list_paths[dataset], n_classes) for dataset in office_list}\n",
    "source = \"amazon\"\n",
    "target = \"dslr\"\n",
    "setting = [source,target]\n",
    "# train_datasets = ConcatDataset([datasets[dataset] for dataset in setting])\n",
    "# len(train_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\"train\": torch.utils.data.DataLoader(datasets[source], batch_size=batch_size, shuffle=True, \n",
    "                                               num_workers=num_workers, pin_memory=True),\n",
    "           \"val\": torch.utils.data.DataLoader(datasets[target], batch_size=batch_size, shuffle=False, \n",
    "                                               num_workers=num_workers, pin_memory=True)}\n",
    "dataset_sizes = {\"train\": len(datasets[source]),\n",
    "                \"val\": len(datasets[target])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    training_memory = {'train': np.zeros(num_epochs),\n",
    "                      'val': np.zeros(num_epochs)}\n",
    "    logger = Logger(num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                print(\"Loss %.2f\" % loss.item())\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            training_memory[phase][epoch] = epoch_acc\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, training_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.alexnet(pretrained=True)\n",
    "classifier = nn.Sequential(model_ft.classifier[:6], nn.Linear(4096, n_classes))\n",
    "model_ft.classifier = classifier\n",
    "# print(model_ft)\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# model_ft.fc = nn.Linear(num_ftrs, n_classes)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "# optimizer_ft = optim.Adam(model_ft.parameters(), lr=3e-4)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=20, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/3\n",
      "----------\n",
      "Loss 3.53\n",
      "Loss 3.56\n",
      "Loss 3.41\n",
      "Loss 3.47\n",
      "Loss 3.55\n",
      "Loss 3.53\n",
      "Loss 3.51\n",
      "Loss 3.49\n",
      "Loss 3.52\n",
      "Loss 3.56\n",
      "Loss 3.49\n",
      "Loss 3.43\n",
      "Loss 3.50\n",
      "Loss 3.47\n",
      "Loss 3.41\n",
      "Loss 3.43\n",
      "Loss 3.48\n",
      "Loss 3.39\n",
      "Loss 3.41\n",
      "Loss 3.36\n",
      "Loss 3.43\n",
      "Loss 3.41\n",
      "Loss 3.76\n",
      "train Loss: 3.4705 Acc: 0.0373\n",
      "Loss 3.49\n",
      "Loss 3.53\n",
      "Loss 3.50\n",
      "Loss 3.50\n",
      "val Loss: 3.5035 Acc: 0.0341\n",
      "\n",
      "Epoch 1/3\n",
      "----------\n",
      "Loss 3.50\n",
      "Loss 3.57\n",
      "Loss 3.47\n",
      "Loss 3.44\n",
      "Loss 3.46\n",
      "Loss 3.44\n",
      "Loss 3.43\n",
      "Loss 3.39\n",
      "Loss 3.46\n",
      "Loss 3.43\n",
      "Loss 3.42\n",
      "Loss 3.37\n",
      "Loss 3.48\n",
      "Loss 3.44\n",
      "Loss 3.39\n",
      "Loss 3.43\n",
      "Loss 3.39\n",
      "Loss 3.39\n",
      "Loss 3.39\n",
      "Loss 3.34\n",
      "Loss 3.39\n",
      "Loss 3.38\n",
      "Loss 4.37\n",
      "train Loss: 3.4276 Acc: 0.0511\n",
      "Loss 3.36\n",
      "Loss 3.39\n",
      "Loss 3.39\n",
      "Loss 3.37\n",
      "val Loss: 3.3778 Acc: 0.0542\n",
      "\n",
      "Epoch 2/3\n",
      "----------\n",
      "Loss 3.34\n",
      "Loss 3.37\n",
      "Loss 3.38\n",
      "Loss 3.37\n",
      "Loss 3.40\n",
      "Loss 3.42\n",
      "Loss 3.39\n",
      "Loss 3.42\n",
      "Loss 3.42\n",
      "Loss 3.42\n",
      "Loss 3.45\n",
      "Loss 3.39\n",
      "Loss 3.38\n",
      "Loss 3.39\n",
      "Loss 3.39\n",
      "Loss 3.40\n",
      "Loss 3.37\n",
      "Loss 3.36\n",
      "Loss 3.36\n",
      "Loss 3.32\n",
      "Loss 3.37\n",
      "Loss 3.40\n",
      "Loss 3.26\n",
      "train Loss: 3.3868 Acc: 0.0472\n",
      "Loss 3.34\n",
      "Loss 3.38\n",
      "Loss 3.37\n",
      "Loss 3.33\n",
      "val Loss: 3.3530 Acc: 0.0643\n",
      "\n",
      "Epoch 3/3\n",
      "----------\n",
      "Loss 3.35\n",
      "Loss 3.37\n",
      "Loss 3.32\n",
      "Loss 3.33\n",
      "Loss 3.34\n",
      "Loss 3.36\n",
      "Loss 3.38\n",
      "Loss 3.38\n",
      "Loss 3.39\n",
      "Loss 3.34\n",
      "Loss 3.33\n",
      "Loss 3.32\n",
      "Loss 3.31\n",
      "Loss 3.34\n",
      "Loss 3.32\n",
      "Loss 3.34\n",
      "Loss 3.28\n",
      "Loss 3.31\n",
      "Loss 3.23\n",
      "Loss 3.29\n",
      "Loss 3.37\n",
      "Loss 3.33\n",
      "Loss 2.07\n",
      "train Loss: 3.3335 Acc: 0.0763\n",
      "Loss 3.27\n",
      "Loss 3.36\n",
      "Loss 3.33\n",
      "Loss 3.39\n",
      "val Loss: 3.3375 Acc: 0.0703\n",
      "\n",
      "Training complete in 0m 36s\n",
      "Best val Acc: 0.070281\n"
     ]
    }
   ],
   "source": [
    "model_ft, memory = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                       num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_plt(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "conv1 = models.alexnet(pretrained=True).features[0] #model_ft.features[0]\n",
    "tmp = conv1.weight.cpu().data\n",
    "tmp = torchvision.utils.make_grid(tmp,normalize=True)\n",
    "plt.imshow(to_plt(tmp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = model_ft.features[0]\n",
    "tmp = conv1.weight.cpu().data\n",
    "tmp = torchvision.utils.make_grid(tmp,normalize=True)\n",
    "plt.imshow(to_plt(tmp))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(memory[\"train\"], label=\"train\")\n",
    "plt.plot(memory[\"val\"], label=\"val\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# iter_c = iter(train_datasets)\n",
    "\n",
    "# for x in range(5):\n",
    "#     tmp = next(iter_c)\n",
    "#     image = to_plt(tmp[0])\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(True)\n",
    "tmp = resnet(torch.Tensor(image.reshape(1,3,225,225)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_plt(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "\n",
    "iter_c = iter(c)\n",
    "\n",
    "for x in range(5):\n",
    "    tmp = next(iter_c)\n",
    "    image = torchvision.utils.make_grid(tmp[2],3)\n",
    "    plt.imshow(to_plt(image))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = models.alexnet(pretrained=True)\n",
    "model_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {\"ciao\":3, \"tu\":1}\n",
    "\", \".join([\"%s : %.2f\" % (k, v) for k,v in losses.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
